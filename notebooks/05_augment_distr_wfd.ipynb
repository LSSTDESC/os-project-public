{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "academic-salon",
   "metadata": {},
   "source": [
    "# Augment distribution properties - WFD\n",
    "\n",
    "In this notebook we computee what are the properties of the test set, and hence which properties the augmented training set should have. We perform this analysis for WFD test set.\n",
    "\n",
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import Packages](#imports)\n",
    "2. [Load Dataset](#loadData)\n",
    "3. [Compute Dataset Distributions](#distr)\n",
    "    1. [Spectroscopic Redshift Distributions](#distrSpecz)\n",
    "    2. [Target Number Observations](#distrNumberObs)\n",
    "    3. [Observations Uncertainty](#distrUnc)\n",
    "        1. [u](#uDistr) [g](#gDistr) [r](#rDistr) [i](#iDistr) [z](#zDistr) [y](#yDistr)\n",
    "    4. [SNR/ Detections](#distrDect)\n",
    "    5. [Number and lenght of Detections](#distrDetect)\n",
    "\n",
    "## 1. Import Packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../snmachine/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from snmachine import snaugment, sndata\n",
    "from utils.plasticc_pipeline import get_directories, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-colonial",
   "metadata": {},
   "source": [
    "#### Aestetic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale=1.3, style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-sheet",
   "metadata": {},
   "source": [
    "## 2. Load Dataset<a name=\"loadData\"></a>\n",
    "\n",
    "First, **write** the path to the folder that contains the dataset we want to augment, `folder_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os_name = 'baseline_v2_0_paper'\n",
    "# os_name = 'noroll_v2_0_paper'\n",
    "os_name = 'presto_v2_0_paper'\n",
    "\n",
    "folder_path = f'/folder/path/'\n",
    "analyses_dir = f'/analyses/path'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-estate",
   "metadata": {},
   "source": [
    "Then, **write** in `data_file_name` the name of the file where your dataset is saved.\n",
    "\n",
    "In this notebook we use the dataset saved in [2_preprocess_data]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_only_roll = 0\n",
    "is_updated = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_name_to_save = 'wfd'\n",
    "\n",
    "file_id = '000'\n",
    "#file_id = '002' # until 009\n",
    "\n",
    "train_data_file_name = f'train_ddf_{extra_name_to_save}_{file_id}_gapless50.pckl'\n",
    "test_data_file_name = f'test_{extra_name_to_save}_{file_id}_gapless50.pckl'\n",
    "if is_only_roll:\n",
    "    train_data_file_name = f'train_ddf_{extra_name_to_save}_{file_id}_roll_gapless50.pckl'\n",
    "    test_data_file_name = f'test_{extra_name_to_save}_{file_id}_roll_gapless50.pckl'\n",
    "if is_updated:\n",
    "    train_data_file_name = train_data_file_name[:-5] + '_updated.pckl'\n",
    "    test_data_file_name = test_data_file_name[:-5] + '_updated.pckl'\n",
    "test_data_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-valuation",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(folder_path, train_data_file_name)\n",
    "train_data = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-hopkins",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "## 3. Compute Dataset Distributions<a name=\"distr\"></a>\n",
    "\n",
    "### 3.1. Spectroscopic Redshift Distributions<a name=\"distrSpecz\"></a>\n",
    "\n",
    "First we check that all the events have spectroscopic redshift. Any event without this value should have been removed in previous notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-staff",
   "metadata": {},
   "source": [
    "Then, we model the redhsift distributions using Gaussian Mixture Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "diverg_color = sns.color_palette(\"Set2\", 6, desat=1)\n",
    "sn_type_color = {42: diverg_color[1], 62: diverg_color[0], 90: diverg_color[2], \n",
    "                 52: diverg_color[3], 67: diverg_color[4], 95: diverg_color[5]}\n",
    "sn_type_name = {42: 'SN II', 62: 'SN Ibc', 90: 'SN Ia', \n",
    "                52: 'SN Iax', 67: 'SN 91bg', 95: 'SLSN'}\n",
    "unique_types = [90, 42, 62, 52, 67, 95]\n",
    "\n",
    "unique_types = [90, 42, 62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 3.2, 50)\n",
    "g = sns.distplot(a=train_metadata['hostgal_photoz'], kde=True, norm_hist=True,\n",
    "                 label='train data')\n",
    "g = sns.distplot(a=test_metadata['hostgal_photoz'], kde=True, norm_hist=True,\n",
    "                 label='test data')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Photometric z')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "for sn_type in unique_types:\n",
    "    is_sn_type = (train_metadata['target'] == sn_type)\n",
    "    sn_type_metadata = train_metadata[is_sn_type]\n",
    "    bins = np.linspace(0, 3.2, 50)\n",
    "    g = sns.distplot(a=sn_type_metadata['hostgal_photoz'], kde=True, hist=False,\n",
    "                     label=sn_type_name[sn_type], color=sn_type_color[sn_type])\n",
    "    \n",
    "    \n",
    "    is_sn_type = (test_metadata['target'] == sn_type)\n",
    "    sn_type_metadata = test_metadata[is_sn_type]\n",
    "    bins = np.linspace(0, 3.2, 50)\n",
    "    g = sns.distplot(a=sn_type_metadata['hostgal_photoz'], kde=True, hist=False, \n",
    "                     color=sn_type_color[sn_type], kde_kws={'ls':'--'})\n",
    "plt.plot([4,5], [0,1], 'k-', label='Train set')\n",
    "plt.plot([4,5], [0,1], 'k--', label='Test set')\n",
    "plt.xlabel('Photometric redshift')\n",
    "plt.ylabel('Density')\n",
    "#plt.title('Diff. between subsets')\n",
    "plt.xlim(-.3, 3.5)\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-tuition",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 3.2. Target Number Observations <a name=\"distrNumberObs\"></a>\n",
    "\n",
    "First we compute the number of observations in each light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_number_obs(dataset):\n",
    "    obj_names = dataset.object_names\n",
    "    number_obs = np.zeros(len(obj_names))\n",
    "    for i in np.arange(len(obj_names)):\n",
    "        obj = obj_names[i]\n",
    "        obj_data = dataset.data[obj].to_pandas()\n",
    "        number_obs[i] = np.shape(obj_data)[0]\n",
    "    return number_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata['number_obs'] = compute_number_obs(train_data)\n",
    "test_metadata['number_obs'] = compute_number_obs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(train_metadata['number_obs']), np.min(test_metadata['number_obs']))\n",
    "print(np.max(train_metadata['number_obs']), np.max(test_metadata['number_obs']))\n",
    "print(np.mean(train_metadata['number_obs']), np.mean(test_metadata['number_obs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-liability",
   "metadata": {},
   "source": [
    "Then, we model the number of observations using Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit = np.array(test_metadata['number_obs']).reshape(len(test_objs), 1)\n",
    "gmm = GaussianMixture(n_components=2, random_state=0).fit(data_to_fit)\n",
    "print(gmm.weights_)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-turkish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 222, 75)\n",
    "g = sns.distplot(a=train_metadata['number_obs'], kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=test_metadata['number_obs'], kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "# Plot the estimated distribution\n",
    "number_points = 100\n",
    "x = np.linspace(0, 222, number_points).reshape(number_points, 1)\n",
    "logprob = gmm.score_samples(x)\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x+2, pdf, '-k', label='GMM fit')\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Total number of observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-scheme",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 3.3. Observations Uncertainty <a name=\"distrUnc\"></a>\n",
    "\n",
    "First we compute the uncertainty in each passband for each light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_big_pb_unc_table(dataset, pb, subset=None):\n",
    "    if subset is None:\n",
    "        obj_names = dataset.object_names\n",
    "    else:\n",
    "        obj_names = dataset.object_names[subset]\n",
    "    metadata = dataset.metadata\n",
    "    unc_pb = []\n",
    "    obs_target = []\n",
    "    for obj in obj_names:\n",
    "        obj_data = dataset.data[obj].to_pandas()\n",
    "        is_pb = obj_data['filter'] == pb\n",
    "        obj_data_pb = obj_data[is_pb]\n",
    "        unc_pb.append(obj_data_pb['flux_error'])\n",
    "        obs_target.append(len(obj_data_pb) * [metadata.loc[obj, 'target']])\n",
    "    unc_pb = pd.concat(unc_pb, ignore_index=True)\n",
    "    obs_target = pd.DataFrame([inner for outer in obs_target for inner in outer])\n",
    "    return unc_pb, obs_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_train = []\n",
    "unc_test = []\n",
    "obs_target_train = []\n",
    "obs_target_test = []\n",
    "for pb in train_data.filter_set:\n",
    "    unc_pb, obs_target_pb = make_big_pb_unc_table(train_data, pb)\n",
    "    unc_train.append(unc_pb)\n",
    "    obs_target_train.append(obs_target_pb)\n",
    "    \n",
    "    unc_pb, obs_target_pb = make_big_pb_unc_table(test_data, pb)\n",
    "    unc_test.append(unc_pb)\n",
    "    obs_target_test.append(obs_target_pb)\n",
    "u_unc_train, g_unc_train, r_unc_train, i_unc_train, z_unc_train, y_unc_train = unc_train\n",
    "u_unc_test, g_unc_test, r_unc_test, i_unc_test, z_unc_test, y_unc_test = unc_test\n",
    "(u_obs_target_train, g_obs_target_train, r_obs_target_train, \n",
    " i_obs_target_train, z_obs_target_train, y_obs_target_train) = obs_target_train\n",
    "(u_obs_target_test, g_obs_target_test, r_obs_target_test, \n",
    " i_obs_target_test, z_obs_target_test, y_obs_target_test) = obs_target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-father",
   "metadata": {},
   "source": [
    "#### 3.3.1. u<a name=\"uDistr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit = np.array(np.log(u_unc_test)).reshape(len(u_unc_test), 1)\n",
    "gmm = GaussianMixture(n_components=2, random_state=0).fit(data_to_fit)\n",
    "print(gmm.weights_)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-signature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(30), 50)\n",
    "g = sns.distplot(a=np.log(u_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(u_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "# Plot the estimated distribution\n",
    "number_points = 500\n",
    "x = np.linspace(-1, 5, number_points).reshape(number_points, 1)\n",
    "logprob = gmm.score_samples(x)\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_choice = np.random.choice(2, size=5000, p=gmm.weights_)\n",
    "gmm_means = gmm.means_\n",
    "gmm_covs = gmm.covariances_\n",
    "\n",
    "gmm_sample = np.zeros(len(gauss_choice))\n",
    "for i in np.arange(2):\n",
    "    is_i = gauss_choice == i\n",
    "    gmm_sample[is_i] = np.random.normal(gmm_means[i], np.sqrt(gmm_covs[i][0]), size=np.sum(is_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-knitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(30), 50)\n",
    "g = sns.distplot(a=np.log(u_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(u_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "s = np.exp(gmm_sample)\n",
    "g = sns.distplot(a=np.log(s), kde=True, norm_hist=True,\n",
    "                 label='log', bins=bins)\n",
    "plt.xlim(np.log(1), np.log(30))\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-stability",
   "metadata": {},
   "source": [
    "#### 3.3.2. g<a name=\"gDistr\"></a>\n",
    "\n",
    "We model the uncertainty of observations using Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(np.log(g_unc_test)), np.max(np.log(g_unc_test)))\n",
    "print(np.min(np.log(g_unc_train)), np.max(np.log(g_unc_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit = np.array(np.log(g_unc_test)).reshape(len(g_unc_test), 1)\n",
    "gmm = GaussianMixture(n_components=2, random_state=0).fit(data_to_fit)\n",
    "print(gmm.weights_)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-secret",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(30), 50)\n",
    "g = sns.distplot(a=np.log(g_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(g_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "# Plot the estimated distribution\n",
    "number_points = 1000\n",
    "x = np.linspace(-1, 5, number_points).reshape(number_points, 1)\n",
    "logprob = gmm.score_samples(x)\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_choice = np.random.choice(2, size=5000, p=gmm.weights_)\n",
    "gmm_means = gmm.means_\n",
    "gmm_covs = gmm.covariances_\n",
    "\n",
    "gmm_sample = np.zeros(len(gauss_choice))\n",
    "for i in np.arange(2):\n",
    "    is_i = gauss_choice == i\n",
    "    gmm_sample[is_i] = np.random.normal(gmm_means[i], np.sqrt(gmm_covs[i][0]), size=np.sum(is_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-graduate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(30), 50)\n",
    "g = sns.distplot(a=np.log(g_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(g_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "s = np.exp(gmm_sample)\n",
    "g = sns.distplot(a=np.log(s), kde=True, norm_hist=True,\n",
    "                 label='log', bins=bins)\n",
    "plt.xlim(-1, 3)\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-huntington",
   "metadata": {},
   "source": [
    "#### 3.3.3. r<a name=\"rDistr\"></a>\n",
    "\n",
    "We model the uncertainty of observations using Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-europe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.min(np.log(r_unc_test)), np.max(np.log(r_unc_test)))\n",
    "print(np.min(np.log(r_unc_train)), np.max(np.log(r_unc_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit = np.array(np.log(r_unc_test)).reshape(len(r_unc_test), 1)\n",
    "gmm = GaussianMixture(n_components=2, random_state=0).fit(data_to_fit)\n",
    "print(gmm.weights_)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-preservation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(50), 50)\n",
    "g = sns.distplot(a=np.log(r_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(r_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "# Plot the estimated distribution\n",
    "number_points = 1000\n",
    "x = np.linspace(-1, 5, number_points).reshape(number_points, 1)\n",
    "logprob = gmm.score_samples(x)\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-attraction",
   "metadata": {},
   "source": [
    "#### 3.3.4. i<a name=\"iDistr\"></a>\n",
    "\n",
    "We model the uncertainty of observations using Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(np.log(i_unc_test)), np.max(np.log(i_unc_test)))\n",
    "print(np.min(np.log(i_unc_train)), np.max(np.log(i_unc_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit = np.array(np.log(i_unc_test)).reshape(len(i_unc_test), 1)\n",
    "gmm = GaussianMixture(n_components=2, random_state=0).fit(data_to_fit)\n",
    "print(gmm.weights_)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-corner",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(30), 50)\n",
    "g = sns.distplot(a=np.log(i_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(i_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "# Plot the estimated distribution\n",
    "number_points = 500\n",
    "x = np.linspace(-1, 5, number_points).reshape(number_points, 1)\n",
    "logprob = gmm.score_samples(x)\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-marketing",
   "metadata": {},
   "source": [
    "#### 3.3.5. z<a name=\"zDistr\"></a>\n",
    "\n",
    "We model the uncertainty of observations using Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(np.log(z_unc_test)), np.max(np.log(z_unc_test)))\n",
    "print(np.min(np.log(z_unc_train)), np.max(np.log(z_unc_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit = np.array(np.log(z_unc_test)).reshape(len(z_unc_test), 1)\n",
    "gmm = GaussianMixture(n_components=1, random_state=0).fit(data_to_fit)\n",
    "print(gmm.weights_)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-syndrome",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(500), 50)\n",
    "g = sns.distplot(a=np.log(z_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(z_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "# Plot the estimated distribution\n",
    "number_points = 500\n",
    "x = np.linspace(-1, 5, number_points).reshape(number_points, 1)\n",
    "logprob = gmm.score_samples(x)\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-metabolism",
   "metadata": {},
   "source": [
    "#### 3.3.6. y<a name=\"yDistr\"></a>\n",
    "\n",
    "We model the uncertainty of observations using Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(np.log(y_unc_test)), np.max(np.log(y_unc_test)))\n",
    "print(np.min(np.log(y_unc_train)), np.max(np.log(y_unc_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fit = np.array(np.log(y_unc_test)).reshape(len(y_unc_test), 1)\n",
    "gmm = GaussianMixture(n_components=1, random_state=0).fit(data_to_fit)\n",
    "print(gmm.weights_)\n",
    "print(gmm.means_)\n",
    "print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-cream",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(500), 50)\n",
    "g = sns.distplot(a=np.log(y_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(y_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "\n",
    "# Plot the estimated distribution\n",
    "number_points = 500\n",
    "x = np.linspace(-1, 5, number_points).reshape(number_points, 1)\n",
    "logprob = gmm.score_samples(x)\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf, '-k')\n",
    "plt.legend()\n",
    "plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-blank",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 3.4. SNR/ Detections <a name=\"distrDect\"></a>\n",
    "\n",
    "First we compute the SNR for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_obs_snr(dataset):\n",
    "    objs = dataset.object_names\n",
    "    obs_flux = []\n",
    "    obs_flux_error = []\n",
    "    for i, obj in enumerate(objs):\n",
    "        obj_data = dataset.data[obj].to_pandas()\n",
    "        obs_flux.append(obj_data['flux'])\n",
    "        obs_flux_error.append(obj_data['flux_error'])\n",
    "    obs_flux = np.concatenate(obs_flux)\n",
    "    obs_flux_error = np.concatenate(obs_flux_error)\n",
    "    obs_snr = obs_flux_error/obs_flux\n",
    "    return obs_snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obs_snr = compute_obs_snr(train_data)\n",
    "test_obs_snr = compute_obs_snr(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_obs_snr = np.log(train_obs_snr)\n",
    "sns.distplot(log_obs_snr, label='train')\n",
    "log_obs_snr = np.log(test_obs_snr)\n",
    "sns.distplot(log_obs_snr, label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-sweet",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 3.5. Number and lenght of detections <a name=\"distrDetect\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_number_len_detect(dataset):\n",
    "    obj_names = dataset.object_names\n",
    "    number_detect = np.zeros(len(obj_names))\n",
    "    len_detect = np.zeros(len(obj_names))\n",
    "    for i in np.arange(len(obj_names)):\n",
    "        obj = obj_names[i]\n",
    "        obj_data = dataset.data[obj]\n",
    "        is_detect = obj_data['detected'] == True\n",
    "        number_detect[i] = np.sum(is_detect)\n",
    "        \n",
    "        obj_mjd_detect = obj_data['mjd'][is_detect]\n",
    "        len_detect[i] = obj_mjd_detect[-1] - obj_mjd_detect[0]\n",
    "    return number_detect, len_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_detect, len_detect = compute_number_len_detect(train_data)\n",
    "train_metadata['number_detect'] = number_detect\n",
    "train_metadata['len_detect'] = len_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_detect, len_detect = compute_number_len_detect(test_data)\n",
    "test_metadata['number_detect'] = number_detect\n",
    "test_metadata['len_detect'] = len_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(train_metadata['number_detect']), np.min(test_metadata['number_detect']))\n",
    "print(np.max(train_metadata['number_detect']), np.max(test_metadata['number_detect']))\n",
    "print(np.mean(train_metadata['number_detect']), np.mean(test_metadata['number_detect']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(train_metadata['len_detect']), np.min(test_metadata['len_detect']))\n",
    "print(np.max(train_metadata['len_detect']), np.max(test_metadata['len_detect']))\n",
    "print(np.mean(train_metadata['len_detect']), np.mean(test_metadata['len_detect']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-concentrate",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
