{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "academic-salon",
   "metadata": {},
   "source": [
    "# Augment dataset\n",
    "\n",
    "In this notebook we exemplify how to augment a dataset. This is tipically done to increase the number and/or representativity of trainining sets.\n",
    "\n",
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import Packages](#imports)\n",
    "2. [Load Dataset](#loadData)\n",
    "    1. [GP Path](#oriGpPath)\n",
    "3. [Augment Dataset](#augData)\n",
    "    1. [Choose the Events to Augment](#chooseEvent)\n",
    "    3. [Choose the Photometric Redshift](#choosePhotoZ)\n",
    "    4. [Run Augmentation](#aug)\n",
    "    5. [See Augmented Dataset Properties](#statsAug)\n",
    "4. [Save Augmented Dataset](#saveAug)\n",
    "5. [Light curve comparison](#comparison)\n",
    "\n",
    "## 1. Import Packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../snmachine/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snmachine import snaugment, sndata\n",
    "from utils.plasticc_pipeline import get_directories, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-colonial",
   "metadata": {},
   "source": [
    "#### Aestetic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale=1.3, style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-sheet",
   "metadata": {},
   "source": [
    "## 2. Load Dataset<a name=\"loadData\"></a>\n",
    "\n",
    "First, **write** the path to the folder that contains the dataset we want to augment, `folder_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os_name = 'baseline_v2_0_paper'\n",
    "os_name = 'noroll_v2_0_paper'\n",
    "# os_name = 'presto_v2_0_paper'\n",
    "\n",
    "folder_path = f'/path/to/save/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-estate",
   "metadata": {},
   "source": [
    "Then, **write** in `data_file_name` the name of the file where your dataset is saved.\n",
    "\n",
    "In this notebook we use the dataset saved in [2_preprocess_data]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_only_roll = 1\n",
    "is_updated = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_name_to_save = 'ddf_wfd'\n",
    "\n",
    "file_id = '000'\n",
    "#file_id = '002' # until 009\n",
    "\n",
    "data_file_name = f'train_{extra_name_to_save}_{file_id}_gapless50.pckl'\n",
    "if is_only_roll:\n",
    "    data_file_name = f'train_{extra_name_to_save}_{file_id}_roll_gapless50.pckl'\n",
    "if is_updated:\n",
    "    data_file_name = data_file_name[:-5] + '_updated.pckl'\n",
    "data_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-valuation",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(folder_path, data_file_name)\n",
    "dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = dataset.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-letters",
   "metadata": {},
   "source": [
    "### 2.1. GP Path<a name=\"oriGpPath\"></a>\n",
    "\n",
    "The GP augmentation uses the previously saved GPs, so **write** the path where they were saved. For help in fitting GPs to the dataset, follow [3_model_lightcurves](3_model_lightcurves.ipynb).\n",
    "\n",
    "**<font color=Orange>A)</font>** Obtain GP path from folder structure.\n",
    "\n",
    "If you created a folder structure, you can obtain the path from there. **Write** the name of the folder in `analysis_name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = data_file_name[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-citizenship",
   "metadata": {},
   "source": [
    "Obtain the required GP path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_analysis_path = folder_path[:-14] + 'analyses'\n",
    "directories = get_directories(folder_analysis_path, analysis_name) \n",
    "path_saved_gps = directories['intermediate_files_directory']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-server",
   "metadata": {},
   "source": [
    "**<font color=Orange>B)</font>** Directly **write** where you saved the GP files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-miami",
   "metadata": {},
   "source": [
    "```python\n",
    "path_saved_gps = os.path.join(folder_path, data_file_name[:-5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-prophet",
   "metadata": {},
   "source": [
    "## 3. Augment Dataset<a name=\"augData\"></a>\n",
    "\n",
    "Here we augment the data and make sure all the properties have the expected values.\n",
    "\n",
    "In the following sections we decide the following augmentation inputs: \n",
    "1. `objs_number_to_aug` : a dictionary specifying which events to augment and by how much.\n",
    "2. `choose_z` : function used to choose the new true redshift of the augmented events.\n",
    "3. `z_table` : dataset containing the spectroscopic and photometric redshift, and photometric redshift error of events; it is used to generate realistic augmented photometric redshifts.\n",
    "4. `max_duration` : maximum duration of the augmented light curves.\n",
    "5. `random_seed` : random seed used; saving this seed allows reproducible results.\n",
    "\n",
    "### 3.1. Choose the Events to Augment<a name=\"chooseEvent\"></a>\n",
    "\n",
    "**Write** in `aug_obj_names` a list containing all the events to augment. Here we will try to augment them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_obj_names = dataset.object_names  # try to augment all events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-raising",
   "metadata": {},
   "source": [
    "**Create** a dictionary that associates to each event, the target number of synthetic events to create from it. Note that some augmentations will fail so this is not the final number of events. Additionally, each class has a different creation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "is_to_aug = np.in1d(dataset.object_names, aug_obj_names)\n",
    "\n",
    "# Choose the target number of events in the augmented dataset. \n",
    "# Usually, only half of this number are accepted in the augmented dataset\n",
    "target_number_aug = np.sum(is_to_aug) * 40\n",
    "\n",
    "number_objs_per_label = collections.Counter(dataset.labels[is_to_aug])\n",
    "number_aug_per_label = target_number_aug//len(number_objs_per_label.keys())\n",
    "objs_number_to_aug = {}\n",
    "for label in number_objs_per_label.keys():\n",
    "    is_label = dataset.labels[is_to_aug] == label\n",
    "    aug_is_label_obj_names = aug_obj_names[is_label]\n",
    "    number_aug_per_obj = number_aug_per_label // np.sum(is_label)\n",
    "    if label == 90:\n",
    "        number_aug_per_obj = int(number_aug_per_obj*.8)\n",
    "    elif label == 95:\n",
    "        number_aug_per_obj = int(number_aug_per_obj*.5)\n",
    "    number_extra_aug_per_obj = number_aug_per_label % np.sum(is_label)\n",
    "    extra_obj = np.random.choice(aug_is_label_obj_names, size=number_extra_aug_per_obj, \n",
    "                                 replace=False)\n",
    "    objs_number_to_aug.update({obj: number_aug_per_obj for obj in aug_is_label_obj_names})\n",
    "    objs_number_to_aug.update({obj: number_aug_per_obj+1 for obj in extra_obj})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We aim to create up to {sum(objs_number_to_aug.values())} events.')  # confirm how many events to create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-privilege",
   "metadata": {},
   "source": [
    "### 3.2. Choose the Photometric Redshift<a name=\"choosePhotoZ\"></a>\n",
    "\n",
    "In order to simulate realistic photometric redshifts for the synthetic events, following [Boone (2019)](https://iopscience.iop.org/article/10.3847/1538-3881/ab5182) we chose a random event from the test set events that had a spectroscopic redshift measurement, and calculated the difference between its spectroscopic and photometric redshifts. We then added this difference to the true redshift of the augmented event to generate a photometric redshift. \n",
    "\n",
    "**Add** such a dataset containing spectroscopic and photometric redshift, and photometric redshift error of events as `z_table`. If none is provided, a similar table is generated from the events in `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_time = time.time() \n",
    "test_data_file_name = f'test_wfd_000_gapless50.pckl'\n",
    "if is_only_roll:\n",
    "    test_data_file_name = f'test_wfd_000_roll_gapless50.pckl'\n",
    "if is_updated:\n",
    "    test_data_file_name = test_data_file_name[:-5] + '_updated.pckl'\n",
    "test_data_path = os.path.join(folder_path, test_data_file_name)\n",
    "print(test_data_path)\n",
    "\n",
    "test_data = load_dataset(test_data_path)\n",
    "test_metadata = test_data.metadata\n",
    "\n",
    "# Discard the events without spectroscopic redshift; \n",
    "# these are encoded with `hostgal_specz` equal to -9\n",
    "z_table = test_metadata[test_metadata.hostgal_specz > -2]\n",
    "print(time.time() - ini_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_table = z_table.append(dataset.metadata[dataset.metadata.hostgal_specz > -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'z_table_' + test_data_file_name\n",
    "if is_only_roll:\n",
    "    file_name = 'z_table_roll_' + test_data_file_name\n",
    "path_to_save_z_table = os.path.join(folder_path, file_name)\n",
    "with open(path_to_save_z_table, 'wb') as path:\n",
    "    pickle.dump(z_table, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-meditation",
   "metadata": {},
   "source": [
    "### 3.3. Run Augmentation<a name=\"aug\"></a>\n",
    "\n",
    "We also need to choose which survey to emulate in the augmentation. At the moment `snmachine` contains the Wide-Fast-Deep (WFD) and the Deep Drilling Field (DDF) survey of the Rubin Observatory Legacy Survey of Space and Time. Use `snaugment.PlasticcWFDAugment` for the former survey and `snaugment.PlasticcDDFAugment` for the latter.\n",
    "You can also implement your own augmentation using those classes as an example.\n",
    "\n",
    "In addition to the above inputs, we **chose** the random seed (`random_seed`) used to allow reproducible results and the maximum duration of the augmented light curves (`max_duration`).\n",
    "\n",
    "The value of `max_duration` must be higher than the maximum duration of any light curve in `dataset`. If none is provided, `max_duration` is set to the length of the longest event in `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The longest event in `dataset` has {dataset.get_max_length():.2f} days.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42 \n",
    "max_duration = 295  # this is the length of the longest event in the paper SNe datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_max_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-resolution",
   "metadata": {},
   "source": [
    "Here we augmented following the DDF survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = snaugment.PrestoV20WFDAugmentOri(dataset=dataset, path_saved_gps=path_saved_gps, \n",
    "                                    objs_number_to_aug=objs_number_to_aug,\n",
    "                                    random_seed=random_seed, max_duration=max_duration, \n",
    "                                    z_table=z_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-pittsburgh",
   "metadata": {},
   "source": [
    "```\n",
    "aug = snaugment.BaselineV20WFDAugment(dataset=dataset, path_saved_gps=path_saved_gps, \n",
    "                                      objs_number_to_aug=objs_number_to_aug,\n",
    "                                      random_seed=random_seed, max_duration=max_duration, \n",
    "                                      z_table=z_table)\n",
    "                                      \n",
    "aug = snaugment.NorollV20WFDAugment(dataset=dataset, path_saved_gps=path_saved_gps, \n",
    "                                    objs_number_to_aug=objs_number_to_aug,\n",
    "                                    random_seed=random_seed, max_duration=max_duration, \n",
    "                                    z_table=z_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug.augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-crowd",
   "metadata": {},
   "source": [
    "Go to:\n",
    "* [Index](#index)\n",
    "* [Save Augmented Dataset](#saveAug)\n",
    "  \n",
    "### 3.4. See Augmented Dataset Properties<a name=\"statsAug\"></a>\n",
    "\n",
    "Here we see some properties of the augmented dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aug_data\n",
    "    aug_metadata\n",
    "    print('Previously loaded')\n",
    "except NameError:\n",
    "    aug_data = aug.only_new_dataset\n",
    "    aug_metadata = aug_data.metadata\n",
    "aug_data = aug.only_new_dataset\n",
    "aug_metadata = aug_data.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # a test set was provided\n",
    "    datasets_label = ['Original', 'Only Aug.', 'Test data']\n",
    "    datasets_metadata = [dataset.metadata, aug_metadata, test_metadata]\n",
    "except NameError:   # no test set was provided\n",
    "    datasets_label = ['Original', 'Only Aug.']\n",
    "    datasets_metadata = [dataset.metadata, aug_metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The longest event in the augmented dataset (`aug.only_new_dataset`)'\n",
    "      f' has {aug_data.get_max_length():.2f} days.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'In total we generated {len(aug_data.object_names)} events.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-grade",
   "metadata": {},
   "source": [
    "Note that we generated less events than our target number of augmented events. As mentioned in Section [Choose the Events to Augment](#chooseEvent), some of the augmentations fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:^12}  {:^12}  {:^12}  {:^12}'.format('Dataset', 'total # objs', '# DDF objs', '% DDF objs'))\n",
    "print('-'*(12*4 + 3*2))\n",
    "for i in np.arange(len(datasets_label)):\n",
    "    is_ddf = datasets_metadata[i]['ddf'] == 1\n",
    "    number_total_objs = len(is_ddf)\n",
    "    number_ddf_objs = np.sum(is_ddf)\n",
    "    print('{:^12} {:^12} {:^12} {:^12.2f}'.format(\n",
    "        datasets_label[i], number_total_objs, number_ddf_objs, \n",
    "        number_ddf_objs/number_total_objs * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(aug.only_new_dataset.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-northeast",
   "metadata": {},
   "source": [
    "We now see the distribution of the photometric redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "diverg_color = sns.color_palette(\"Set2\", 6, desat=1)\n",
    "sn_type_color = {42: diverg_color[1], 62: diverg_color[0], 90: diverg_color[2]}\n",
    "sn_type_name = {42: 'SN II', 62: 'SN Ibc', 90: 'SN Ia'}\n",
    "unique_types = [90, 42, 62] #, 52, 67, 95]\n",
    "datasets_ls = ['-', '-', '--']\n",
    "datasets_linewidth = [1, 3, 3]\n",
    "datasets_bw_adjust = [.3, .4, .4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1.5, 65)\n",
    "for sn_type in unique_types: # sns scale 2\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i, metadata in enumerate(datasets_metadata):\n",
    "        label = datasets_label[i]\n",
    "        ls = datasets_ls[i]\n",
    "        linewidth = datasets_linewidth[i]\n",
    "        bw_adjust = datasets_bw_adjust[i]\n",
    "        is_sn_type = (metadata['target'] == sn_type)\n",
    "        sn_type_metadata = metadata[is_sn_type]\n",
    "        spec_zs = sn_type_metadata['hostgal_specz']\n",
    "        if np.min(spec_zs) < 0:\n",
    "            print(label)\n",
    "            spec_zs = sn_type_metadata['sim_redshift_cmb']\n",
    "        sns.distplot(a=spec_zs, bins=bins,\n",
    "                     label=label, \n",
    "                     kde_kws={'linestyle': ls, \n",
    "                              'linewidth': linewidth, \n",
    "                              'bw_adjust': bw_adjust})\n",
    "    sn_name = sn_type_name[sn_type]\n",
    "    plt.title('trap .1*2/d log(z) z aug\\n'+sn_name)\n",
    "    plt.xlabel('Simulated redshift')\n",
    "    plt.legend()\n",
    "    plt.xlim(0, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-uzbekistan",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "#### 3.4.2. Target Number Observations <a name=\"distrNumberObs\"></a>\n",
    "\n",
    "We compute the number of observations in each light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_number_obs(dataset):\n",
    "    obj_names = dataset.object_names\n",
    "    number_obs = np.zeros(len(obj_names))\n",
    "    for i in np.arange(len(obj_names)):\n",
    "        obj = obj_names[i]\n",
    "        obj_data = dataset.data[obj].to_pandas()\n",
    "        number_obs[i] = np.shape(obj_data)[0]\n",
    "    return number_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_number_obs = compute_number_obs(dataset)\n",
    "aug_metadata['number_obs'] = compute_number_obs(aug_data)\n",
    "test_metadata['number_obs'] = compute_number_obs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(train_number_obs), np.min(aug_metadata['number_obs']), np.min(test_metadata['number_obs']))\n",
    "print(np.max(train_number_obs), np.max(aug_metadata['number_obs']), np.max(test_metadata['number_obs']))\n",
    "print(np.mean(train_number_obs), np.mean(aug_metadata['number_obs']), np.mean(test_metadata['number_obs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 250, 51)\n",
    "g = sns.distplot(a=train_number_obs, kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=test_metadata['number_obs'], kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "g = sns.distplot(a=aug_metadata['number_obs']+2, kde=True, norm_hist=True,\n",
    "                 label='aug. train', bins=bins)\n",
    "plt.legend()\n",
    "#plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Total number of observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 250, 51)\n",
    "g = sns.distplot(a=train_number_obs, kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins, hist=False)\n",
    "g = sns.distplot(a=test_metadata['number_obs'], kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins, hist=False)\n",
    "g = sns.distplot(a=aug_metadata['number_obs'], kde=True, norm_hist=True,\n",
    "                 label='aug. train', bins=bins, hist=False)\n",
    "plt.legend()\n",
    "#plt.title(f'All SN')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Total number of observations')\n",
    "# aug noroll has 6obs less in general than the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_test = analysis.compute_lc_length(test_data)\n",
    "lc_aug = analysis.compute_lc_length(aug.only_new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(a=lc_test, kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins, hist=False)\n",
    "g = sns.distplot(a=lc_aug, kde=True, norm_hist=True,\n",
    "                 label='aug. train', bins=bins, hist=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-bathroom",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "#### 3.4.3. Observations Uncertainty <a name=\"distrUnc\"></a>\n",
    "\n",
    "First we compute the uncertainty in each passband for each light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_big_pb_unc_table(dataset, pb, subset=None):\n",
    "    if subset is None:\n",
    "        obj_names = dataset.object_names\n",
    "    else:\n",
    "        obj_names = dataset.object_names[subset]\n",
    "    metadata = dataset.metadata\n",
    "    unc_pb = []\n",
    "    obs_target = []\n",
    "    for obj in obj_names:\n",
    "        obj_data = dataset.data[obj].to_pandas()\n",
    "        is_pb = obj_data['filter'] == pb\n",
    "        obj_data_pb = obj_data[is_pb]\n",
    "        unc_pb.append(obj_data_pb['flux_error'])\n",
    "        obs_target.append(len(obj_data_pb) * [metadata.loc[obj, 'target']])\n",
    "    unc_pb = pd.concat(unc_pb, ignore_index=True)\n",
    "    obs_target = pd.DataFrame([inner for outer in obs_target for inner in outer])\n",
    "    return unc_pb, obs_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_train = []\n",
    "unc_test = []\n",
    "unc_aug = []\n",
    "obs_target_train = []\n",
    "obs_target_test = []\n",
    "obs_target_aug = []\n",
    "for pb in dataset.filter_set:\n",
    "    unc_pb, obs_target_pb = make_big_pb_unc_table(dataset, pb)\n",
    "    unc_train.append(unc_pb)\n",
    "    obs_target_train.append(obs_target_pb)\n",
    "    \n",
    "    unc_pb, obs_target_pb = make_big_pb_unc_table(test_data, pb)\n",
    "    unc_test.append(unc_pb)\n",
    "    obs_target_test.append(obs_target_pb)\n",
    "    \n",
    "    unc_pb, obs_target_pb = make_big_pb_unc_table(aug_data, pb)\n",
    "    unc_aug.append(unc_pb)\n",
    "    obs_target_aug.append(obs_target_pb)\n",
    "u_unc_train, g_unc_train, r_unc_train, i_unc_train, z_unc_train, y_unc_train = unc_train\n",
    "u_unc_test, g_unc_test, r_unc_test, i_unc_test, z_unc_test, y_unc_test = unc_test\n",
    "u_unc_aug, g_unc_aug, r_unc_aug, i_unc_aug, z_unc_aug, y_unc_aug = unc_aug\n",
    "(u_obs_target_train, g_obs_target_train, r_obs_target_train, \n",
    " i_obs_target_train, z_obs_target_train, y_obs_target_train) = obs_target_train\n",
    "(u_obs_target_test, g_obs_target_test, r_obs_target_test, \n",
    " i_obs_target_test, z_obs_target_test, y_obs_target_test) = obs_target_test\n",
    "(u_obs_target_aug, g_obs_target_aug, r_obs_target_aug, \n",
    " i_obs_target_aug, z_obs_target_aug, y_obs_target_aug) = obs_target_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(np.log(.4), np.log(30), 50)\n",
    "g = sns.distplot(a=np.log(u_unc_train), kde=True, norm_hist=True,\n",
    "                 label='train', bins=bins)\n",
    "g = sns.distplot(a=np.log(u_unc_test), kde=True, norm_hist=True,\n",
    "                 label='test', bins=bins)\n",
    "g = sns.distplot(a=np.log(u_unc_aug), kde=True, norm_hist=True,\n",
    "                 label='aug. train', bins=bins)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('u passband')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_colors = {'lsstu':'#984ea3', 'lsstg':'#377eb8', 'lsstr':'#4daf4a', \n",
    "             'lssti':'#e3c530', 'lsstz':'#ff7f00', 'lssty':'#e41a1c'} # colours for the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets_flux_error, pb = [u_unc_train, u_unc_aug, u_unc_test], 'lsstu'\n",
    "#datasets_flux_error, pb = [g_unc_train, g_unc_aug, g_unc_test], 'lsstg'\n",
    "#datasets_flux_error, pb = [r_unc_train, r_unc_aug, r_unc_test], 'lsstr'\n",
    "#datasets_flux_error, pb = [i_unc_train, i_unc_aug, i_unc_test], 'lssti'\n",
    "# datasets_flux_error, pb = [z_unc_train, z_unc_aug, z_unc_test], 'lsstz'\n",
    "datasets_flux_error, pb = [y_unc_train, y_unc_aug, y_unc_test], 'lssty'\n",
    "datasets_label = ['Train set unc.', 'Aug. set unc.', 'Test set unc.']\n",
    "datasets_ls = ['-', '-', '--']\n",
    "datasets_linewidth = [1, 3, 3]\n",
    "\n",
    "bins = np.linspace(np.log(.4), np.log(30), 50)\n",
    "\n",
    "for i, metadata in enumerate(datasets_flux_error):\n",
    "    label = datasets_label[i]\n",
    "    ls = datasets_ls[i]\n",
    "    linewidth = datasets_linewidth[i]\n",
    "    sns.distplot(a=np.log(metadata), kde=True, color=pb_colors[pb],\n",
    "                 hist=False, label=label, bins=bins,\n",
    "                 kde_kws={'linestyle':ls, 'linewidth':linewidth,\n",
    "                          'bw_adjust':.7})\n",
    "#sn_name = sn_type_name[sn_type]\n",
    "plt.title(f'Passband {pb}')\n",
    "plt.xlim(-1, 4) # g with DDF train\n",
    "#plt.ylim(0, 3)\n",
    "#plt.xscale('log')\n",
    "plt.xlabel('Log(Flux uncertainty)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(handletextpad=.3)\n",
    "plt.legend(handletextpad=.3, borderaxespad=.3, handlelength=1,\n",
    "       labelspacing=.2, borderpad=.3, columnspacing=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-figure",
   "metadata": {},
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_space(metadata, title, y_feature_name, ylabel, ylim, yscale):\n",
    "    fig, axs = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(7, 7),\n",
    "                            gridspec_kw={'hspace': 0, 'wspace': 0, \n",
    "                                         'width_ratios': [4, 1], \n",
    "                                         'height_ratios': [1, 4]})\n",
    "    (ax1, ax2), (ax3, ax4) = axs\n",
    "    fig.suptitle(title, y=.94)\n",
    "    n_bins = 10000\n",
    "    for sn_type in unique_types:\n",
    "        is_sn_type = (metadata['target'] == sn_type)\n",
    "        sn_type_metadata = metadata[is_sn_type]\n",
    "        y_feature = sn_type_metadata[y_feature_name]\n",
    "        ax3.plot(sn_type_metadata['hostgal_photoz'], \n",
    "                 y_feature, alpha=.1, \n",
    "                 linestyle='', marker='.', \n",
    "                 color=sn_type_color[sn_type])\n",
    "        ax3.plot(0, 1, 'o', color=sn_type_color[sn_type], \n",
    "                 label=sn_type_name[sn_type])\n",
    "\n",
    "        ax1.hist(sn_type_metadata['hostgal_photoz'], n_bins, density=True, \n",
    "                 histtype='step', cumulative=True, label='CDF', linewidth=1.5, \n",
    "                 color=sn_type_color[sn_type])\n",
    "\n",
    "        ax4.hist(y_feature, n_bins, density=True, histtype='step', \n",
    "                 cumulative=True, label='CDF', linewidth=1.5, orientation='horizontal', \n",
    "                 color=sn_type_color[sn_type])\n",
    "\n",
    "    ax1.set_ylim(-.1, 1.1)\n",
    "    ax1.set_ylabel('CDF')\n",
    "    ax4.set_xlim(-.1, 1.1)\n",
    "    ax4.set_xlabel('CDF')\n",
    "    ax4.set_xticks([0., .5, 1.])\n",
    "\n",
    "    ax3.legend(handletextpad=.3, borderaxespad=.3, labelspacing=.2, \n",
    "               borderpad=.2, columnspacing=.4)\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.set_yscale(yscale)\n",
    "    ax3.set_xlim(.01, 4)\n",
    "    ax3.set_ylim(ylim)\n",
    "    ax3.set_xlabel('Photometric z')\n",
    "    ax3.set_ylabel(ylabel)\n",
    "\n",
    "    fig.delaxes(axs[0][1])\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_flux(dataset, return_time=False):\n",
    "    \"\"\"Max flux in all pbs\"\"\"\n",
    "    obj_names = dataset.object_names\n",
    "    pbs = dataset.filter_set\n",
    "    max_flux = pd.DataFrame(index=obj_names, columns=pbs, dtype=float)\n",
    "    time_max_flux = pd.DataFrame(index=obj_names, columns=pbs, dtype=float)\n",
    "    \n",
    "    for obj in obj_names:\n",
    "        obj_gps = dataset.models[obj].to_pandas()\n",
    "        for pb in pbs:\n",
    "            is_pb = obj_gps['filter'] == pb\n",
    "            obj_pb = obj_gps[is_pb].reset_index()\n",
    "            max_flux.loc[obj, pb] = np.max(obj_pb['flux'])\n",
    "            if return_time:\n",
    "                index_max = np.argmax(obj_pb['flux'])\n",
    "                time_max_flux.loc[obj, pb] = obj_pb.loc[index_max, 'mjd']\n",
    "    if return_time:\n",
    "        return max_flux, time_max_flux\n",
    "    return max_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flux_train = compute_max_flux(dataset)\n",
    "max_flux_aug = compute_max_flux(aug_data)\n",
    "max_flux_test = compute_max_flux(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-charles",
   "metadata": {},
   "source": [
    "### 3.5. Select subset of Augmented Dataset<a name=\"selectAug\"></a>\n",
    "\n",
    "Here we see select a subset of the augmented data to make it balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = aug.only_new_dataset\n",
    "aug_metadata = aug_data.metadata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = [90, 42, 62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(aug.only_new_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "objs_to_keep = []\n",
    "\n",
    "for sn_type in unique_types:\n",
    "    is_sn = aug_metadata.target == sn_type\n",
    "    indexes = np.where(is_sn)[0]\n",
    "    try:\n",
    "        if i == 44: # placeholder for when I only want 1 class\n",
    "            indexes_to_stay = indexes\n",
    "        else:\n",
    "            indexes_to_stay = np.random.choice(indexes,\n",
    "                                               size=15440,\n",
    "                                               replace=False)\n",
    "    except ValueError:\n",
    "        print(f'The class {aug_metadata.target[is_sn][0]} only has {len(indexes)} events.')\n",
    "        indexes_to_stay = indexes\n",
    "    objs_to_stay = is_sn[indexes_to_stay].index.to_numpy()\n",
    "    objs_to_keep.append(objs_to_stay)\n",
    "objs_to_keep = np.concatenate(objs_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata = aug_metadata.loc[objs_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data.object_names = list(new_metadata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data.update_dataset(list(new_metadata.index))\n",
    "aug_data.update_dataset(list(aug_data.metadata.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(aug_data.metadata['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aug_data.object_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aug_data.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1262*3)\n",
    "print(2782*3)\n",
    "print(6256*3)\n",
    "print(18928*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-satellite",
   "metadata": {},
   "source": [
    "## 4. Save Augmented Dataset<a name=\"saveAug\"></a>\n",
    "\n",
    "Now, we save the `PlasticcData` instance containing only the augmented events. **Chose** a path to save (`folder_path_to_save`) and the name of the file (`file_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_to_save = folder_path[:-9] + 'augmented_data/'\n",
    "file_name = 'aug_wfd_46k_updated.pckl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-cherry",
   "metadata": {},
   "source": [
    "At this point we could also choose to save only part of the augmented dataset. Here we save all the augmented events.\n",
    "\n",
    "**Add** an extra step to select your chosen subset. See the notebook [1_load_data](1_load_data.ipynb) for a tutorial on how to select a subset from a `PlasticcData` instance. This can be used, for example, to create an augmented training set with the same number of events in each class. For a working example of how to balance the augmented training set, see the notebook [example_plasticc](example_plasticc.ipynb).\n",
    "\n",
    "Finally, save the `PlasticcData` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only_aug_dataset = aug.only_new_dataset\n",
    "only_aug_dataset = aug_data\n",
    "\n",
    "path_to_save = os.path.join(folder_path_to_save, file_name)\n",
    "with open(path_to_save, 'wb') as f:\n",
    "    pickle.dump(only_aug_dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-elite",
   "metadata": {},
   "source": [
    "## 5. Light curve visualization<a name=\"see\"></a>\n",
    "\n",
    "Here we show the light curve of an event along with one of the synthetic events generated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_show = '595791'\n",
    "sndata.PlasticcData.plot_obj_and_model(dataset.data[obj_show])\n",
    "photo_z = dataset.metadata.loc[obj_show, 'hostgal_photoz']\n",
    "plt.title(f'Event {obj_show}; z = {photo_z:.3f}')\n",
    "print(dataset.metadata.loc[obj_show, 'hostgal_photoz'], \n",
    "      dataset.metadata.loc[obj_show, 'hostgal_specz'])\n",
    "obj_data = dataset.data[obj_show] \n",
    "print(obj_data[obj_data['detected']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_aug_show = obj_show + '_aug32'\n",
    "sndata.PlasticcData.plot_obj_and_model(only_aug_dataset.data[obj_aug_show])\n",
    "photo_z = only_aug_dataset.metadata.loc[obj_aug_show, 'hostgal_photoz']\n",
    "plt.title(f'Event {obj_aug_show}; z = {photo_z:.3f}')\n",
    "print(only_aug_dataset.metadata.loc[obj_aug_show, 'hostgal_photoz'], \n",
    "      only_aug_dataset.metadata.loc[obj_aug_show, 'hostgal_specz'])\n",
    "obj_data = only_aug_dataset.data[obj_aug_show] \n",
    "print(obj_data[obj_data['detected']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = only_aug_dataset.data[obj_aug_show]\n",
    "oo[oo['detected']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-concentrate",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
