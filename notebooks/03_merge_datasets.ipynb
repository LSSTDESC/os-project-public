{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bridal-craps",
   "metadata": {},
   "source": [
    "# Merge datasets\n",
    "\n",
    "\n",
    "In this notebook we exemplify how to merge two previously generated `SnanaData` datasets. In particular, we are going to merge a DDF and a WFD `SnanaData` datasets.\n",
    "\n",
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import packages](#imports)\n",
    "2. [Load datasets](#data)\n",
    "    1. [Dataset 1](#data1)\n",
    "    2. [Dataset 2](#data2)\n",
    "    3. [Diagnostics](#diagPre) <font color=salmon>(Optional)</font>\n",
    "3. [Merge datasets](#merge)\n",
    "    1. [Diagnostics](#diagPost)\n",
    "4. [Save SnanaData instance](#save)\n",
    "    1. [Load SnanaData instance](#load) <font color=salmon>(Optional)</font>\n",
    "\n",
    "## 1. Import packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../snmachine/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # if we need to reload any module do `reload(module_to_reload)`\n",
    "from snmachine import sndata\n",
    "from utils.plasticc_pipeline import get_directories, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-magic",
   "metadata": {},
   "source": [
    "#### Aestetic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale=1.3, style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-lewis",
   "metadata": {},
   "source": [
    "## 2. Load datasets<a name=\"data\"></a>\n",
    "\n",
    "First, we need to **write** the path to the folder where the dataset and metadata are, `folder_path`. If the files are not in the same folder, add the correct path in each subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os_name = 'baseline_v2_0_paper'\n",
    "# os_name = 'noroll_v2_0_paper'\n",
    "os_name = 'presto_v2_0_paper'\n",
    "\n",
    "folder_path = f'/folder/path/'\n",
    "\n",
    "subset_name = 'train' # we only need to merge the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-ordering",
   "metadata": {},
   "source": [
    "### 2.1. Dataset 1<a name=\"data1\"></a>\n",
    "\n",
    "**Write** the name of the dataset and its metadata, respectively `data_file_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_roll = 0\n",
    "is_updated = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_name_to_save_1 = 'ddf'\n",
    "\n",
    "file_id = '000'\n",
    "\n",
    "data_file_name = f'{subset_name}_{extra_name_to_save_1}_{file_id}_gapless50.pckl'\n",
    "if is_roll:\n",
    "    data_file_name = f'{subset_name}_{extra_name_to_save_1}_{file_id}_roll_gapless50.pckl'\n",
    "if is_updated:\n",
    "    data_file_name = data_file_name[:-5] + '_updated.pckl'\n",
    "data_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-paraguay",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(folder_path, data_file_name)\n",
    "dataset_1 = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_1 = dataset_1.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the ddf info if the datasets do not have it yet\n",
    "try:\n",
    "    meta_1.ddf\n",
    "except AttributeError:\n",
    "    print('Add ddf column to metadata.')\n",
    "    meta_1['ddf'] = extra_name_to_save_1 == 'ddf'\n",
    "    dataset_1.metadata = meta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-gardening",
   "metadata": {},
   "source": [
    "### 2.2. Dataset 2<a name=\"data2\"></a>\n",
    "\n",
    "**Write** the name of the dataset and its metadata, respectively `data_file_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_name_to_save_2 = 'wfd'\n",
    "\n",
    "file_id = '000'\n",
    "\n",
    "data_file_name = f'{subset_name}_{extra_name_to_save_2}_{file_id}_gapless50.pckl'\n",
    "if is_roll:\n",
    "    data_file_name = f'{subset_name}_{extra_name_to_save_2}_{file_id}_roll_gapless50.pckl'\n",
    "if is_updated:\n",
    "    data_file_name = data_file_name[:-5] + '_updated.pckl'\n",
    "data_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-punch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(folder_path, data_file_name)\n",
    "dataset_2 = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_2 = dataset_2.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the ddf info if the datasets do not have it yet\n",
    "try:\n",
    "    meta_2.ddf\n",
    "except AttributeError:\n",
    "    print('Add ddf column to metadata.')\n",
    "    meta_2['ddf'] = extra_name_to_save_2 == 'ddf'\n",
    "    dataset_2.metadata = meta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-iceland",
   "metadata": {},
   "source": [
    "Go to:\n",
    "* [Index](#index)\n",
    "* [Merge datasets](#merge)\n",
    "\n",
    "### 2.3. Diagnostics<a name=\"diagPre\"></a> <font color=salmon>(Optional)</font>\n",
    "\n",
    "Here we simply look at some statistics to ensure the above datasets are what we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{extra_name_to_save_1} {subset_name} ;', \n",
    "      collections.Counter(dataset_1.metadata['target']), len(dataset_1.metadata))\n",
    "print(f'{extra_name_to_save_2} {subset_name} ;', \n",
    "      collections.Counter(dataset_2.metadata['target']), len(dataset_2.metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-allah",
   "metadata": {},
   "source": [
    "PLAsTiCC-like Baseline v2.0 only differs from PLAsTiCC in the cadence. Hence, we expect the relationship between the number of events to be similar to the one of the solid angle.\n",
    "\n",
    "I obtain twice the number of DDF training set events due to the doubling of the solid angle value in Baseline v2.0 cadence. The solid angle has twice the value for DDF due to the dithers and there is also a small contribution due to correctly adding in the WFDs that fall into the DDF area. I am happy with this, so I will now add the WFD and DDF training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DDF')\n",
    "print('Baseline v2.0/ PLAsTiCC')\n",
    "print(0.030/0.01451)\n",
    "print(len(dataset_1.metadata)/1270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WFD')\n",
    "print('Baseline v2.0/ PLAsTiCC')\n",
    "print(5.694/5.468)\n",
    "print(len(dataset_2.metadata)/2720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-surname",
   "metadata": {},
   "source": [
    "We now check the position on the sky of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(meta_2.ra, meta_2.dec, '.', label='WFD')\n",
    "plt.plot(meta_1.ra, meta_1.dec, '.', label='DDF')\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('DEC')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-expense",
   "metadata": {},
   "source": [
    "Check the redshift distribution per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-triangle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets_metadata = [meta_1, meta_2]\n",
    "datasets_label = ['DDF', 'WFD']\n",
    "datasets_bw_adjust = [.3, .3]\n",
    "datasets_colors = ['C0', 'C1']\n",
    "sn_type_name = {42: 'SN II', 62: 'SN Ibc', 90: 'SN Ia',\n",
    "                95: 'SLSN-I', 67: 'SN Ia-91bg', 52: 'SN Iax'}\n",
    "unique_types = [90, 42, 62]\n",
    "unique_types2 = [90, 42, 62, 95, 67, 52]\n",
    "\n",
    "bins = np.linspace(0, 2.2, 50)\n",
    "\n",
    "for sn_type in unique_types:\n",
    "    plt.figure()\n",
    "    for i, metadata in enumerate(datasets_metadata):\n",
    "        label = datasets_label[i]\n",
    "        bw_adjust = datasets_bw_adjust[i]\n",
    "        \n",
    "        is_sn_type = (metadata['target'] == sn_type)\n",
    "        sn_type_metadata = metadata[is_sn_type]\n",
    "        sns.distplot(a=sn_type_metadata['hostgal_photoz'], kde=True,\n",
    "                     hist=True, label=label, color=datasets_colors[i],\n",
    "                     bins=bins, kde_kws={'bw_adjust':bw_adjust})\n",
    "    sn_name = sn_type_name[sn_type]\n",
    "    plt.title('WFD+DDF train set\\n'+sn_name)\n",
    "    plt.xlim(-.1, 1.2)\n",
    "    #plt.ylim(0, 3.6)\n",
    "    plt.xlabel('Photometric redshift')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend(handletextpad=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-adult",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "## 3. Merge datasets<a name=\"merge\"></a>\n",
    "\n",
    "First, we merge the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = sndata.EmptyDataset().merge_dataset(dataset_1, dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-blogger",
   "metadata": {},
   "source": [
    "### 3.1. Diagnostics<a name=\"diagPost\"></a>\n",
    "\n",
    "Now, we do some diagnostics to ensure the datasets were successfully merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of events')\n",
    "print('Before')\n",
    "number_1 = len(dataset_1.object_names)\n",
    "number_2 = len(dataset_2.object_names)\n",
    "print(f'{extra_name_to_save_1:<10}: {number_1} {np.shape(meta_1)}')\n",
    "print(f'{extra_name_to_save_2:<10}: {number_2} {np.shape(meta_2)}')\n",
    "name_total = 'total'\n",
    "print(f'{name_total:<10}: {number_1+number_2}')\n",
    "print(35*'-')\n",
    "print('After')\n",
    "meta_new = new_dataset.metadata\n",
    "is_ddf = meta_new.ddf == 1\n",
    "print(f'{extra_name_to_save_1:<10}: {np.sum(is_ddf)}')\n",
    "print(f'{extra_name_to_save_2:<10}: {np.sum(~is_ddf)}')\n",
    "print(f'{name_total:<10}: {len(is_ddf)} {np.shape(meta_new)}')\n",
    "print(f'{name_total:<10}: {len(new_dataset.object_names)} {len(new_dataset.data)}')\n",
    "print(35*'-')\n",
    "print('If the before and after are the same, the merge was successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-reader",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diverg_color = sns.color_palette(\"Set2\", 6, desat=1)\n",
    "sn_type_color = {42: diverg_color[1], 62: diverg_color[0], 90: diverg_color[2],\n",
    "                 95: diverg_color[3], 67: diverg_color[4], 52: diverg_color[5]}\n",
    "\n",
    "for sn_type in unique_types:\n",
    "    plt.figure()\n",
    "    is_sn_type = (meta_new['target'] == sn_type)\n",
    "    sn_type_metadata = meta_new[is_sn_type]\n",
    "    sns.distplot(a=sn_type_metadata['hostgal_photoz'], kde=True,\n",
    "                 hist=True, label=label, color=sn_type_color[sn_type],\n",
    "                 bins=bins, kde_kws={'bw_adjust':.3})\n",
    "    sn_name = sn_type_name[sn_type]\n",
    "    plt.title('WFD+DDF merged train set\\n'+sn_name)\n",
    "    plt.xlim(-.1, 1.2)\n",
    "    #plt.ylim(0, 3.6)\n",
    "    plt.xlabel('Photometric redshift')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend(handletextpad=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-fight",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "## 4. Save SnanaData instance<a name=\"save\"></a>\n",
    "\n",
    "Now, **choose** a path to save the `SnanaData` instance created (`folder_path_to_save`) and the name of the file (`file_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_to_save = folder_path\n",
    "file_name = f'train_{extra_name_to_save_1}_{extra_name_to_save_2}_{file_id}_gapless50.pckl'\n",
    "if is_roll:\n",
    "    file_name = f'train_{extra_name_to_save_1}_{extra_name_to_save_2}_{file_id}_roll_gapless50.pckl'\n",
    "if is_updated:\n",
    "    file_name = file_name[:-5] + '_updated.pckl'\n",
    "file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-smoke",
   "metadata": {},
   "source": [
    "Finally, save the `SnanaData` instance. \n",
    "\n",
    "For the PLAsTiCC-like Baseline v2.0 train set it takes 10s to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = os.path.join(folder_path_to_save, file_name)\n",
    "print(f'File to save in {path_to_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start_saving = time.time()\n",
    "with open(path_to_save, 'wb') as f:\n",
    "    pickle.dump(new_dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "print(f'{time.time() - time_start_saving}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-merit",
   "metadata": {},
   "source": [
    "### 4.1 Load SnanaData instance<a name=\"load\"></a> <font color=salmon>(Optional)</font>\n",
    "\n",
    "We can load the saved file to verify weather it was correctly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start_saving = time.time()\n",
    "saved_dataset = load_dataset(path_to_save)\n",
    "print(f'{time.time() - time_start_saving}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = saved_dataset.metadata\n",
    "np.unique(metadata.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['hostgal_photoz', 'hostgal_photoz_err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(saved_dataset.metadata[numerical_cols], \n",
    "            new_dataset.metadata[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-prophet",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
